# Bittle + RTX 3050最適化設定
environment:
  name: "BittleWalking-v0"
  max_episode_steps: 2000  # エピソード長を延長（学習時間制限なし）
  control_frequency: 50
  physics_frequency: 240
  gravity: -9.81
  render_mode: None
  
robot:
  urdf_path: "bittle.urdf"
  initial_position: [0.0, 0.0, 0.12]  # Bittleの実際の高さに調整
  initial_orientation: [0.0, 0.0, 0.0]
  joint_limits: [-1.047, 1.047]  # ±60度（Bittleの実際の制限）
  max_torque: 0.2  # Bittleの実際のトルク（0.2Nm）
  
rewards:
  # VecNormalizeが正規化するため、生の値で設定
  forward_velocity_weight: 10.0      # 生の速度値（m/s）に適用
  survival_reward: 1.0               # 固定値
  fall_penalty: -50.0                # 転倒時のペナルティ
  energy_efficiency_weight: -0.01    # 生の角度変化量（rad）に適用
  orientation_stability_weight: 0.5  # 生の角度誤差（rad）に適用
  
  # 削除された報酬要素（VecNormalizeと競合するため）
  # target_height: 0.12
  # height_stability_weight: 12.0
  # vertical_velocity_penalty: 5.0
  
termination:
  max_roll: 80.0     # 緩い設定
  max_pitch: 80.0    # 緩い設定

training:
  algorithm: "PPO"
  total_timesteps: 10000000  # 1000万ステップ（学習時間制限なし）
  n_envs: 8                  # 並列環境数を増加（RTX 3050対応）
  n_steps: 1024             # ステップ数を増加
  learning_rate: 0.0003     # 学習率（安定学習）
  batch_size: 32            # バッチサイズを削減（RTX 3050のVRAM制限対応）
  n_epochs: 8               # エポック数を増加
  gamma: 0.995              # 割引率を上げる（長期報酬重視）
  gae_lambda: 0.98          # GAEラムダを上げる
  clip_range: 0.2           # クリップ範囲
  ent_coef: 0.01            # エントロピー係数
  vf_coef: 0.5              # 価値関数係数
  max_grad_norm: 0.5        # 勾配クリッピング
  
save:
  frequency: 200000        # 20万ステップごとに保存（長時間学習対応）
  model_path: "./models"
  checkpoint_path: "./models/checkpoints"
  vec_normalize_path: "./models/vec_normalize.pkl"
  
evaluation:
  frequency: 100000        # 10万ステップごとに評価
  n_eval_episodes: 5       # 評価エピソード数を増加
  deterministic: true
  
logging:
  log_dir: "./logs"
  tensorboard: true
  verbose: 1
  debug_level: "INFO"      # デバッグレベルを調整